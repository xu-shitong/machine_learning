\documentclass[UTF8]{ctexart}
\usepackage{geometry}
\usepackage{amsmath}
\usepackage{graphicx} %插入图片的宏包
\usepackage{float} %设置图片浮动位置的宏包
\usepackage{setspace}
\geometry{a4paper,scale=0.8}
\sectionfont{\bfseries\Large\raggedright}

\title{torch笔记}
\author{徐世桐}
\date{}
\begin{document}
\maketitle

% ----------------------------------------------------------------------
% |                              import                                |
% ----------------------------------------------------------------------
\section{import}
\noindent \texttt{from torch import nn, optim}\\
\texttt{import torch.nn.functional as F}\\
\texttt{from torchvision import models, transforms}\\
\texttt{from PIL import Image}

% ----------------------------------------------------------------------
% |                           tensor使用GPU                             |
% ----------------------------------------------------------------------
\section{tensor使用GPU}
\noindent \texttt{}
\texttt{if torch.cuda.is\_available():}\\
\texttt{  dev = "cuda:0"}\\
\texttt{else:}\\
\texttt{  dev = "cpu"}\\
\texttt{device = torch.device(dev)}

  得到device数据类型，定义使用CPU或使用哪一GPU\\
\texttt{Tensor.get\_device()}

  查看张量存在的CPU/GPU



% ----------------------------------------------------------------------
% |                           tensor数据类型                             |
% ----------------------------------------------------------------------
\section{tensor数据类型}
\noindent \texttt{torch.arange}

  \texttt{torch.arange(a)} 得到Tensor$[0, 1, ..., \lfloor a \rfloor]$

  \texttt{torch.arange(a, b)} 得到Tensor$[a, a+1, ..., a+n]$，n为整数且$a+n < b$

  \texttt{torch.arange(a, b, c)} 得到Tensor$[a, a+c, ..., a+nc]$，n为整数且$a+nc < b$
  
  \texttt{torch.arange(..., requires\_grad=True)} 分配空间记录斜率，同mxnet的attach\_grad()
  
  \texttt{torch.arange(..., device=device数据类型)} 将张量分配进指定CPU/GPU\\
\texttt{torch.tensor([], REQUIRES\_GRAD, DEVICE)}
  
  通过python数组创建Tensor

  REQUIRES\_GRAD = \texttt{True} 分配空间记录斜率
  
  DEVICE = \texttt{device数据类型} 将张量分配进指定CPU/GPU\\
\texttt{torch.numpy()} 得到NDArray \\
\texttt{torch.tolist()} 得到python 数组 \\
\texttt{torch.from\_numpy(NDArray)} 从\texttt{NDArray}创建Tensor\\
\texttt{torch.mm(Tensor, Tensor)} tensor矩阵乘法\\
\texttt{torch.mm(Tensor, Tensor)} 矩阵\ 向量乘法\\
\texttt{torch.matmul(Tensor, Tensor)} 任意张量间乘法，包含广播机制\\
\texttt{+-*/} 同NDArray使用广播机制\\
\texttt{Tensor.reshape()} 改变形状，\textbf{新形状元素数必须等于输入元素数}\\
\texttt{Tensor.permute(SHAPE)} 等同mxnet的transpose，更改纬度顺序\\
\texttt{Tensor.transpose(dim0, dim1)} 交换两个纬度\\
\texttt{torch.sum(Tensor, *DIM)} 将某一纬度一下的值求和\\
\texttt{Tensor.to(device)} 将张量分配进指定CPU/GPU\\
\texttt{torch.random(MEAN, STD, SIZE*)}

  \texttt{size=($x_1, x_2, ...$)} 限定输出张量形状

  \texttt{mean=Tensor}, \texttt{std=Tensor/const} 当没有限定size时\texttt{mean}必为float Tensor，形状和输出形状相同。

  \texttt{mean=Tensor/const}, \texttt{std=Tensor/const} 当限定size后\texttt{mean, std}可为const或单个值的Tensor\\
\texttt{torch.rand(SIZE*)}

  得到SIZE形状的随机数张量，每一元素$\in [0,1)$。SIZE无定义则得到const随机数

  代替torch.uniform功能\\
\texttt{dataset = torch.utils.data.TensorDataset(样本Tensor, 标签Tensor)}\\
\texttt{dataiter = torch.utils.data.DataLoader(dataset, batch\_size=批量大小, shuffle=True)}

  使用torch进行批量迭代
  
  dataiter输出的feature，label使用的CPU/GPU\ 和样本Tensor，标签Tensor使用的CPU/GPU分别对应\\
\texttt{torch.save(Tensor, '文件名')} 文件中保存一张量\\
\texttt{Tensor = torch.load('文件名')} 读取文件中张量\\
\texttt{torch.cuda.synchronize()} 等待异步计算结束，打印结果同样等待异步计算

% ----------------------------------------------------------------------
% |                           torch神经网络                              |
% ----------------------------------------------------------------------
\section{torch神经网络}
\noindent \texttt{net = nn.Sequential()}\\
神经网络定义：\texttt{net.add\_module('层名', 层)}\\
层定义：

  \texttt{nn.Linear(输入节点数，输出节点数)} 定义全连接层

  \texttt{}\\
\textbf{可使用层直接进行前向计算，训练函数中使用[layer.weight, net.bias]传入参数}\\
\textbf{前向计算为(|B|, 特征数) 和\ 权重\ 矩阵相乘}\\
\textbf{使用GPU时层定义后需加.to(device), 并不可以使用device=赋GPU}\\
\texttt{net.weight/bias.data.fill\_(值)} 对\textbf{层}中所有权重/偏差赋\textbf{同一}值\\
\texttt{net.weight/bias = nn.Parameter(Tensor)} 将参数初始化为指定张量\\
\texttt{nn.init.xavier\_uniform(net.weight/.bias)} 对\textbf{层}中所有权重/偏差使用xavier初始化\\
\texttt{loss}

  \texttt{= nn.MSELoss(REDUCTION*)} 平方代价函数

  \quad REDUCTION = \texttt{'none' | 'mean' | 'sum'} 得到每一样本代价值向量 | 得到平均代价 | 得到代价和。默认为'mean'

  \texttt{= nn.CrossEntropyLoss()} 交叉熵损失函数，\textbf{已经包含softmax计算}\\
\texttt{trainer}

  \texttt{= optim.SGD(net.parameters(), lr=学习率)} SGD迭代函数

  \texttt{= optim.Adam(net.parameters(), lr=学习率)} Adam-SGD迭代

  \texttt{trainer.step()} 进行迭代
  
  \textbf{每一迭代中trainer.grad\_zero()清零斜率，否则训练斜率为随机值，代价值在某一高值波动}\\
\texttt{net.parameters()} 得到权重

  \texttt{list(net.parameters())} 得到param类型数组，包含\texttt{[第一层权重, 第一层偏差, ..., 最后一层参数]}
  
  \texttt{param类型数组.data} 得到参数张量
  
  \texttt{param类型数组.name} 得到所属层名，可为空\\
\texttt{loss(y\_hat, y).backward()} 得到代价函数值，求导

  不会调用\texttt{.sum()} 或\texttt{.mean()}，求和方法在loss函数中定义\\
\texttt{class out\_image(nn.Module):}

  \texttt{def \_\_init\_\_(self):}

  \quad \texttt{super().\_\_init\_\_()}

  \texttt{def forward(self, x):}

  \quad 自定义神经网络\\
\texttt{net = models.NET\_NAME(pretrained=True)} 得到预训练的神经网络\\


% ----------------------------------------------------------------------
% |                             读取图像                                 |
% ----------------------------------------------------------------------
\section{读取图像}
\noindent \texttt{image = Image.open('图像路径'))}

  得到图片，显示图片直接调取\texttt{image.show()}，显示结果不阻断python程序。\\
\texttt{transform = transforms.Compose([trans1, trnas2, ...])}

  合并多个对图像的变换

  \texttt{transforms.Resize(图片形状)} 缩放图片

  \texttt{transforms.ToTensor()} 图片变为张量

  \texttt{transforms.Normalize(MEAN, STD)} 对图片的张量输入，求标准化。MEAN，STD可为张量
  
  \quad 内部实现：对RGB 3通道上的像素分别使用(3, )形状的MEAN，STD值求标准化\\
\texttt{transform(image)} 使用transform\\
\texttt{显示图片}

  \texttt{image = transforms.ToPILImage()(image\_tensor)}

  \texttt{image.show()}
  
  \quad 或使用自定义包，支持反标准化

  \texttt{import sys}

  \texttt{sys.path.append('../machine\_learning/')}

  \texttt{from utils.functions import show\_tensor\_image, un\_normalize\_image}

  \texttt{show\_tensor\_image(un\_normalize\_image(image.reshape((3, height, width)), image\_mean, image\_std))}

% ----------------------------------------------------------------------
% |                             常见错误                                 |
% ----------------------------------------------------------------------
\section{常见错误}
\noindent 调用\texttt{trainer.zero\_grad()}

  否则参数代价值高，并迭代后不下降\\
使用网络层作为权重，不参与斜率计算时\ 调用\texttt{layer.requires\_grad\_(False)}

  否则调用\texttt{loss.backward()}时提示需要\texttt{retain\_graph=True}\\
\texttt{transforms.ToPILImage()}不保证像素值在\texttt{[0,1]}区间，需调用\texttt{image\_tensor.clamp(min=0, max=1)}

  d2lzh自动对图像做clip，保证值在\texttt{[0,1]}区间
  
  否则图像中包含突出像素点，如红\ 紫\ 蓝像素。\\
循环中更改\texttt{Tensor}值并将\texttt{Tensor}加入数组，使用\texttt{Tensor.clone()}复制斜率

  否则下一迭代可能更改上一迭代已经加入数组的张量\\
前向计算中不能调用\texttt{Tensor.detach()}

  否则此项无法求斜率，无法进行迭代

\end{document}
