\documentclass[UTF8]{ctexart}
\usepackage{geometry}
\usepackage{amsmath}
\usepackage{graphicx} %插入图片的宏包
\usepackage{float} %设置图片浮动位置的宏包
\usepackage{setspace}
\geometry{a4paper,scale=0.8}
\sectionfont{\bfseries\Large\raggedright}

\title{torch笔记}
\author{徐世桐}
\date{}
\begin{document}
\maketitle

% ----------------------------------------------------------------------
% |                              import                                |
% ----------------------------------------------------------------------
\section{import}
\noindent \texttt{from torch import nn}\\
\texttt{import torch.nn.functional as F}\\
\texttt{import torch.optim as optim}

% ----------------------------------------------------------------------
% |                           tensor使用GPU                             |
% ----------------------------------------------------------------------
\section{tensor使用GPU}
\noindent \texttt{}
\texttt{if torch.cuda.is\_available():}\\
\texttt{  dev = "cuda:0"}\\
\texttt{else:}\\
\texttt{  dev = "cpu"}\\
\texttt{device = torch.device(dev)}

  得到device数据类型，定义使用CPU或使用哪一GPU\\
\texttt{Tensor.get\_device()}

  查看张量存在的CPU/GPU



% ----------------------------------------------------------------------
% |                           tensor数据类型                             |
% ----------------------------------------------------------------------
\section{tensor数据类型}
\noindent \texttt{torch.arange}

  \texttt{torch.arange(a)} 得到Tensor$[0, 1, ..., \lfloor a \rfloor]$

  \texttt{torch.arange(a, b)} 得到Tensor$[a, a+1, ..., a+n]$，n为整数且$a+n < b$

  \texttt{torch.arange(a, b, c)} 得到Tensor$[a, a+c, ..., a+nc]$，n为整数且$a+nc < b$
  
  \texttt{torch.arange(..., requires\_grad=True)} 分配空间记录斜率，同mxnet的attach\_grad()
  
  \texttt{torch.arange(..., device=device数据类型)} 将张量分配进指定CPU/GPU\\
\texttt{torch.tesnor([], REQUIRES\_GRAD, DEVICE)}
  
  通过python数组创建Tensor

  REQUIRES\_GRAD = \texttt{True} 分配空间记录斜率
  
  DEVICE = \texttt{device数据类型} 将张量分配进指定CPU/GPU\\
\texttt{torch.from\_numpy(NDArray)} 从\texttt{NDArray}创建Tensor\\
\texttt{torch.mm(Tensor, Tensor)} tensor矩阵乘法\\
\texttt{+-*/} 同NDArray使用广播机制\\
\texttt{Tensor.reshape()} 改变形状，\textbf{新形状元素数必须等于输入元素数}\\
\texttt{Tensor.to(device)} 将张量分配进指定CPU/GPU\\
\texttt{torch.random(MEAN, STD, SIZE*)}

  \texttt{size=($x_1, x_2, ...$)} 限定输出张量形状

  \texttt{mean=Tensor}, \texttt{std=Tensor/const} 当没有限定size时\texttt{mean}必为float Tensor，形状和输出形状相同。

  \texttt{mean=Tensor/const}, \texttt{std=Tensor/const} 当限定size后\texttt{mean, std}可为const或单个值的Tensor\\
\texttt{torch.rand(SIZE*)}

  得到SIZE形状的随机数张量，每一元素$\in [0,1)$。SIZE无定义则得到const随机数

  代替torch.uniform功能\\
\texttt{dataset = torch.utils.data.TensorDataset(样本Tensor, 标签Tensor)}\\
\texttt{dataiter = torch.utils.data.DataLoader(dataset, batch\_size=批量大小, shuffle=True)}

  使用torch进行批量迭代
  
  dataiter输出的feature，label使用的CPU/GPU\ 和样本Tensor，标签Tensor使用的CPU/GPU分别对应\\
\texttt{torch.save(Tensor, '文件名')} 文件中保存一张量\\
\texttt{Tensor = torch.load('文件名')} 读取文件中张量

% ----------------------------------------------------------------------
% |                           torch神经网络                              |
% ----------------------------------------------------------------------
\section{torch神经网络}
\noindent \texttt{net = nn.Sequential()}\\
神经网络定义：\texttt{net.add\_module('层名', 层)}\\
层定义：

  \texttt{nn.Linear(输入节点数，输出节点数)} 定义全连接层

  \texttt{}\\
\textbf{可使用层直接进行前向计算，训练函数中使用[layer.weight, net.bias]传入参数}\\
\textbf{前向计算为(|B|, 特征数) 和\ 权重\ 矩阵相乘}\\
\textbf{使用GPU时层定义后需加.to(device), 并不可以使用device=赋GPU}\\
\texttt{net.weight/bias.data.fill\_(值)} 对\textbf{层}中所有权重/偏差赋值\\
\texttt{nn.init.xavier\_uniform(net.weight/.bias)} 对\textbf{层}中所有权重/偏差使用xavier初始化\\
\texttt{loss = nn.MSELoss()} 平方代价函数\\
\texttt{trainer = optim.SGD(net.parameters(), lr=学习率)} SGD迭代函数

  trainer.step() 进行迭代
  
  \textbf{每一迭代中trainer.grad\_zero()清零斜率，否则训练斜率为随机值，代价值在某一高值波动}\\
\texttt{net.parameters()} 得到权重

  \texttt{list(net.parameters())} 得到param类型数组，包含\texttt{[第一层权重, 第一层偏差, ..., 最后一层参数]}
  
  \texttt{param类型数组.data} 得到参数张量
  
  \texttt{param类型数组.name} 得到所属层名，可为空\\
\texttt{loss(y\_hat, y).backward()} 得到代价函数值，求导

  

  


\end{document}
