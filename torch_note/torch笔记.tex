\documentclass[UTF8]{ctexart}
\usepackage{geometry}
\usepackage{amsmath}
\usepackage{graphicx} %插入图片的宏包
\usepackage{float} %设置图片浮动位置的宏包
\usepackage{setspace}
\geometry{a4paper,scale=0.8}
\sectionfont{\bfseries\Large\raggedright}

\title{torch笔记}
\author{徐世桐}
\date{}
\begin{document}
\maketitle

% ----------------------------------------------------------------------
% |                              import                                |
% ----------------------------------------------------------------------
\section{import}
\noindent \texttt{from torch import nn, optim}\\
\texttt{import torch.nn.functional as F}\\
\texttt{from torchvision import models, transforms}\\
\texttt{from PIL import Image}\\
\texttt{from numpy.random import default\_rng}\\
\textbf{import 自定义函数}

  python\_file：调用import的python 文件

  程序path：python\_file 的文件夹path

  运行path：执行\texttt{python python\_file}时终端所在的path

  import path：被import的程序path (包含文件名)

  1. 当程序path和运行path相同

  \quad \texttt{from directory. ... .python\_file import func}

  2. 当程序path和运行path不同时

  \quad \texttt{import sys}

  \quad \texttt{sys.path.append(PATH1)}

  \quad \texttt{from PATH2 import func}

  \quad \quad 保证PATH1 + PATH2 = import path即可

  \quad \textbf{sys.path中已包含程序path，但不可使用import ..path得到\ 不为程序path子文件夹内\ 的程序}

  3. 在jupyter-notebook 中运行时

  \quad 方法1：2. 中path需包含至运行path最底文件夹位置

  \quad 方法2：代码同2. 在import 中的path每一文件夹下创建空\texttt{\_\_init\_\_.py}文件，重启jupyter kernel后运行

  得到\textbf{运行path}：

  \quad \texttt{import os}

  \quad \texttt{path = os.path.abspath(os.path.join('.'))}

  得到\textbf{现有sys path}：

  \quad \texttt{import sys}

  \quad \texttt{path = sys.path}

  \textbf{import path：作用为在每一 sys.path 后连接path}


% ----------------------------------------------------------------------
% |                           tensor使用GPU                             |
% ----------------------------------------------------------------------
\section{tensor使用GPU}
\noindent \texttt{}
\texttt{if torch.cuda.is\_available():}\\
\texttt{  dev = "cuda:0"}\\
\texttt{else:}\\
\texttt{  dev = "cpu"}\\
\texttt{device = torch.device(dev)}

  得到device数据类型，定义使用CPU或使用哪一GPU\\
\texttt{Tensor.get\_device()}

  查看张量存在的CPU/GPU



% ----------------------------------------------------------------------
% |                           tensor数据类型                             |
% ----------------------------------------------------------------------
\section{tensor数据类型}
\noindent \texttt{torch.arange}

  \texttt{torch.arange(a)} 得到Tensor$[0, 1, ..., \lfloor a \rfloor]$

  \texttt{torch.arange(a, b)} 得到Tensor$[a, a+1, ..., a+n]$，n为整数且$a+n < b$

  \texttt{torch.arange(a, b, c)} 得到Tensor$[a, a+c, ..., a+nc]$，n为整数且$a+nc < b$
  
  \texttt{torch.arange(..., requires\_grad=True)} 分配空间记录斜率，同mxnet的attach\_grad()
  
  \texttt{torch.arange(..., device=device数据类型)} 将张量分配进指定CPU/GPU\\
\texttt{torch.tensor([], REQUIRES\_GRAD, DEVICE)}
  
  通过python数组创建Tensor

  REQUIRES\_GRAD = \texttt{True} 分配空间记录斜率
  
  DEVICE = \texttt{device数据类型} 将张量分配进指定CPU/GPU\\
\texttt{torch.numpy()} 得到numpy array \\
\texttt{torch.tolist()} 得到python 数组 \\
\texttt{Torch.item()} 当张量中仅有一个元素，得到此元素 \\
\texttt{torch.from\_numpy(NDArray)} 从\texttt{NDArray}创建Tensor\\
\texttt{torch.mm(Tensor, Tensor)} tensor矩阵乘法\\
\texttt{torch.mm(Tensor, Tensor)} 矩阵\ 向量乘法\\
\texttt{torch.matmul(Tensor, Tensor)} 

  任意张量间乘法
  
  \textbf{张量间乘法：两张量最后两维符合矩阵乘法，高维使用广播机制广播矩阵}\\
\texttt{+-*/} 

  同NDArray使用广播机制
  
  \textbf{广播机制要求：形状从最后一维开始，每一维元素/张量个数相同\ 或\ 其中一个数为1}\\
\texttt{torch.sum(Tensor, *DIM)} 将某一纬度一下的值求和\\
\texttt{torch.max(Tensor, *DIM)} 

  当没有指定维度DIM，求所有\texttt{Tensor}元素最大值
  
  当给定纬度DIM，\textbf{返回(Tensor 1, Tensor 2)元组}。第一张量为给定维度最大元素，第二张量为给定维度最大元素index\\
\texttt{Tensor[:, ..., None]} 
  
  '[]'中前n位置为':'，则在第n\ 至\ 第n+1维间插入一维

  则形状由$(s_1, ..., s_n, s_{n+1}, ...)$转为$(s_1, ..., s_n, 1, s_{n+1}, ...)$
  
  可使用多个None加入多个1元素维\\
\texttt{Tensor.unsqueeze(dim*)}

  在第dim-1至dim维加入一维，同\texttt{Tensor[(: * dim), None]}\\
\texttt{Tensor.squeeze(dim)}

  若第dim维为1元素维(即\texttt{Tensor.shape[dim] == 1})，则删除此维。

  若dim为None，则将所有1元素维删除\\
\texttt{Tensor.reshape()} 改变形状，\textbf{新形状元素数必须等于输入元素数}\\
\texttt{Tensor.permute(SHAPE)} 等同mxnet的transpose，更改纬度顺序\\
\texttt{Tensor.transpose(dim0, dim1)} 交换两个纬度\\
\texttt{torch.vstack([Tensor, Tensor, ...])}在第0维方向连接张量，(对矩阵即在竖直方向上连接)\\
\texttt{torch.hstack([Tensor, Tensor, ...])}在第1维方向连接张量，(对矩阵即在水平方向上连接)

  \textbf{即vstack的形状第0位置值可不同，hstack形状第1位置可不同}

  在循环中加入行得到张量，初始化张量为\texttt{torch.tensor([]).reshape((.., 0, ..))}\\
\texttt{Tensor.nonzero()} 得到一list的index向量$\{v_i\}$，$Tensor[v_i]$为非零值\\
\texttt{Tensor.to(device)} 将张量分配进指定CPU/GPU\\
\texttt{Tensor = Tensor.type(torch.float64)} \texttt{Np.astype('float')} 转换类型\\
\texttt{F.one\_hot(Tensor, NUM\_CLASS)} 

  将Tensor中每一元素转为NUM\_CLASS长度的onehot向量，NUM\_CLASS默认为Tensor中最大值\\
\texttt{torch.normal(MEAN, STD, SIZE*)}

  \texttt{size=($x_1, x_2, ...$)} 限定输出张量形状

  \texttt{mean=Tensor}, \texttt{std=Tensor/const} 当没有限定size时\texttt{mean}必为float Tensor，形状和输出形状相同。

  \texttt{mean=Tensor/const}, \texttt{std=Tensor/const} 当限定size后\texttt{mean, std}可为const或单个值的Tensor\\
\texttt{torch.rand(SIZE*)}

  得到SIZE形状的随机数张量，每一元素$\in [0,1)$。SIZE无定义则得到const随机数

  代替torch.uniform功能\\
\texttt{dataset = torch.utils.data.TensorDataset(样本Tensor, 标签Tensor)}\\
\texttt{dataiter = torch.utils.data.DataLoader(dataset, batch\_size=批量大小, shuffle=True)}

  使用torch进行批量迭代
  
  dataiter输出的feature，label使用的CPU/GPU\ 和样本Tensor，标签Tensor使用的CPU/GPU分别对应\\
\texttt{torch.save(Tensor, '文件名')} 文件中保存一张量\\
\texttt{Tensor = torch.load('文件名')} 读取文件中张量\\
\texttt{torch.cuda.synchronize()} 等待异步计算结束，打印结果同样等待异步计算\\
\texttt{random\_generator = default\_rng()}\\
\texttt{shuffled\_index = random\_generator.permutation(列表大小)}

  得到乱序index 列表

% ----------------------------------------------------------------------
% |                           torch神经网络                              |
% ----------------------------------------------------------------------
\section{torch神经网络}
\noindent \texttt{net = nn.Sequential()}\\
神经网络定义：\texttt{net.add\_module('层名', 层)}\\
层定义：

  \texttt{nn.Linear(输入节点数，输出节点数)} 定义全连接层
  
  \quad \textbf{当全连接层输入为2维以上的张量时，全连接层仅对最低维操作。即最低维元素数同输入节点数}\\
\textbf{可使用层直接进行前向计算，训练函数中使用[layer.weight, net.bias]传入参数}\\
\textbf{前向计算为(|B|, 特征数) 和\ 权重\ 矩阵相乘}\\
\textbf{使用GPU时层定义后需加.to(device), 并不可以使用device=赋GPU}\\
\texttt{net.weight/bias.data.fill\_(值)} 对\textbf{层}中所有权重/偏差赋\textbf{同一}值\\
\texttt{net.weight/bias = nn.Parameter(Tensor)} 将参数初始化为指定张量，\textbf{参数参与反向传播，作为.parameters()的输出之一}\\
\texttt{nn.init.xavier\_uniform\_(net.weight/.bias)} 对\textbf{层}中所有权重/偏差使用xavier初始化\\
\texttt{nn.init.normal\_(net.weight/.bias, MEAN, STD)} 对\textbf{层}使用normal初始化\\
\texttt{def init\_func(layer):}

  \texttt{if isinstance(layer, nn.Linear):}

  \quad \texttt{// 根据上一条笔记更新layer的参数}\\
\texttt{net.apply(init\_func)} // 对每一层参数初始化权重\ 偏差\\
\texttt{rnn = nn.RNN(...)}
  
  INPUT\_SIZE：一样本特征数
  
  HIDDEN\_SIZE：RNN 隐藏层neuron数
  
  NUM\_LAYERS：RNN 隐藏层数，每层隐藏层都有HIDDEN\_SIZE neuron。default 1
  
  NONLINEARITY：激活函数，可\texttt{'relu'}或\texttt{'tanh'}字符创。default为\texttt{'tanh'}

  BIAS：hidden计算中是否使用$b_x$和$b_h$。default为true

  BATCH\_FIRST：false时输入X为(时间步数，批量大小，特征数)，true时X为(批量大小，时间步数，特征数)。default为false

  DROPOUT：定义每一隐藏层后dropout 层的几率，default为0。即不使用dropout

  BIDIRECTIONAL：是否为双向神经网络，default为false
  
  \textbf{rnn不可用.weight/.bias取参数}\\
\texttt{rnn(X, H)}

  计算$H, H_n = \sigma(XW_{xh} + b_x + H_nW_{hh} + b_h)$

  \quad X形状见RNN BATCH\_SIZE变量说明

  \quad H形状(隐藏层数，时间步数，每层neuron数，特征数)。双向RNN中第一维值$\times$2

  \quad $H_n$为所有隐藏层的输出，
  
  \textbf{输出仅为隐藏状态，不包含全连接层计算}

  \textbf{$H$为参与最后全连接层计算的输出，即所有时间步的隐藏状态。}
  
  \textbf{$H_n$为参与下一次计算的隐藏状态张量，即最后一时间步的隐藏状态，与$H$中最后一张量相同}\\
\texttt{loss}

  \texttt{= nn.MSELoss(REDUCTION*)} 平方代价函数

  \quad REDUCTION = \texttt{'none' | 'mean' | 'sum'} 得到每一样本代价值向量 | 得到平均代价 | 得到代价和。默认为'mean'

  \texttt{= nn.CrossEntropyLoss()} catagorical交叉熵损失函数，\textbf{已经包含softmax计算}
  
  \texttt{= nn.BCELoss()} 二元交叉熵损失函数
  
  \texttt{= nn.BCEWithLogitsLoss()} 包含sigmoid的二元交叉熵损失函数，数值稳定性更高\\
\texttt{trainer}

  \texttt{= optim.SGD(net.parameters(), lr=学习率)} SGD迭代函数

  \texttt{= optim.Adam(net.parameters(), lr=学习率)} Adam-SGD迭代

  \texttt{trainer.step()} 进行迭代
  
  \textbf{每一迭代中trainer.grad\_zero()清零斜率，否则训练斜率为随机值，代价值在某一高值波动}\\
\texttt{net.parameters()} 得到权重

  \texttt{list(net.parameters())} 得到param类型数组，包含\texttt{[第一层权重, 第一层偏差, ..., 最后一层参数]}
  
  \texttt{param类型数组.data} 得到参数张量
  
  \texttt{param类型数组.name} 得到所属层名，可为空\\
\texttt{loss(y\_hat, y).backward()} 得到代价函数值，求导

  不会调用\texttt{.sum()} 或\texttt{.mean()}，求和方法在loss函数中定义\\
\texttt{Tensor.detach()}

  当此Tensor作为另一神经网络输入时，detach导致此张量不参与反向传播。即不对此张量和得到此张量的计算求导\\
\texttt{Tensor.require\_grad=False}

  前向计算中仍记录数值用于反向计算，但调用\texttt{step()}不会更新参数\\
\texttt{class out\_image(nn.Module):}

  \texttt{def \_\_init\_\_(self):}

  \quad \texttt{super().\_\_init\_\_()}

  \texttt{def forward(self, x):}

  \quad 自定义神经网络\\
\texttt{net = models.NET\_NAME(pretrained=True)} 得到预训练的神经网络

% ----------------------------------------------------------------------
% |                             读取图像                                 |
% ----------------------------------------------------------------------
\section{读取图像}
\noindent \texttt{image = Image.open('图像路径'))}

  得到图片，显示图片直接调取\texttt{image.show()}，显示结果不阻断python程序。\\
\texttt{transform = transforms.Compose([trans1, trnas2, ...])}

  合并多个对图像的变换

  \texttt{transforms.Resize(图片形状)} 缩放图片

  \texttt{transforms.ToTensor()} 图片变为张量

  \texttt{transforms.Normalize(MEAN, STD)} 对图片的张量输入，求标准化。MEAN，STD可为张量
  
  \quad 内部实现：对RGB 3通道上的像素分别使用(3, )形状的MEAN，STD值求标准化\\
\texttt{transform(image)} 使用transform\\
\texttt{显示图片}

  \texttt{image = transforms.ToPILImage()(image\_tensor)}

  \texttt{image.show()}
  
  或使用自定义包，支持反标准化

  \texttt{import sys}

  \texttt{sys.path.append('../machine\_learning/')}

  \texttt{from utils.functions import show\_tensor\_image, un\_normalize\_image}

  \texttt{show\_tensor\_image(un\_normalize\_image(image.reshape((3, height, width)), image\_mean, image\_std))}

  显示多行图片

  \quad \texttt{grids = make\_grid([Tensor], nrow, padding)}

  \quad \texttt{plt.imshow(grids.permute((1, 2, 0)))}

  \quad \texttt{plt.show()}
  
  \quad \quad nrow定义一行图片个数

  \quad \quad padding定义连接2图像时间隔像素数

  \quad \quad make\_grid得到(n, c, h, w)形状张量，即n张图片，每一图使用c通道。返回将所有图片连接结果，形状为(3, h', w')



% ----------------------------------------------------------------------
% |                             常见错误                                 |
% ----------------------------------------------------------------------
\section{常见错误}
\noindent 调用\texttt{trainer.zero\_grad()}

  否则参数代价值高，并迭代后不下降\\
使用网络层作为权重，不参与斜率计算时\ 调用\texttt{layer.requires\_grad\_(False)}

  否则调用\texttt{loss.backward()}时提示需要\texttt{retain\_graph=True}，由于反向传播在错误的试图更新网络权重\\
\texttt{transforms.ToPILImage()}不保证像素值在\texttt{[0,1]}区间，需调用\texttt{image\_tensor.clamp(min=0, max=1)}

  d2lzh自动对图像做clip，保证值在\texttt{[0,1]}区间
  
  否则图像中包含突出像素点，如红\ 紫\ 蓝像素。\\
循环中更改\texttt{Tensor}值并将\texttt{Tensor}加入数组，使用\texttt{Tensor.clone()}复制斜率

  否则下一迭代可能更改上一迭代已经加入数组的张量\\
前向计算中不能调用\texttt{Tensor.detach()}

  否则此项无法求斜率，无法进行迭代\\
\texttt{retain\_graph=True}

  当一次前向计算保存的记录需要被再次使用，设置retain\_graph=True 

  由于调用\texttt{.backward()}后前向传播记录被清空

\end{document}
