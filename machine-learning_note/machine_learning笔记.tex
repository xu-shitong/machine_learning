\documentclass[UTF8]{ctexart}
\usepackage{geometry}
\usepackage{amsmath}
\usepackage{graphicx} %插入图片的宏包
\usepackage{float} %设置图片浮动位置的宏包
\geometry{a4paper,scale=0.8}
\sectionfont{\bfseries\Large\raggedright}

\title{machine learning笔记}
\author{徐世桐}
\date{}
\begin{document}
\maketitle

% ----------------------------------------------------------------------
% |                              基础定义                               |
% ----------------------------------------------------------------------
\section{基础定义}
\noindent \textbf{二元分类}：输出分类个数为2\\
\textbf{多元分类}：输出分类个数不限

  $one-versus-the-rest$ OvR：计算属于每一分类的可能性，取可能性最大的分类为输出分类

  $one-versus-one$ OvO：对所有分类两两使用二元分类，每一分类器训练只需一部分数据\\
\textbf{multilabel多标签分类}：目标检测，对一图像中的物体加label\\
\textbf{multioutput多类分类}：多标签分类，每一标签可包含多种信息\\
\textbf{learning schedule}：根据迭代次数更新学习率\\
\textbf{early stopping}：提早结束训练

  对于每一epoch，当验证集MSE值增高时，证明开始overfit，停止训练

  即在epoch-error图中泛化误差最低时停止训练\\
\textbf{在训练中使用正则化代价函数，训练结束后测试中代价函数不使用正则化项}

% ----------------------------------------------------------------------
% |                              数学计算                               |
% ----------------------------------------------------------------------
\section{数学计算}
\noindent \textbf{MSE} = $\frac{1}{m}\sum_{i=1}^{m}(x^{(i)} - \bar{x} )^2$\\
\textbf{rigid regression}：回归方法，$J(\theta) = MSE(\theta) + \frac{\alpha}{2}\sum_{i}\theta_i^2$

  降低所有权重值\\
\textbf{lasso regression}：回归方法，$J(\theta) = MSE(\theta) + \alpha \sum_i |\theta_i|$

  降低不重要的权重值\\
\textbf{elastic net}：回归方法，$J(\theta) = MSE(\theta) + \gamma\alpha \sum_i |\theta_i| + (1-\gamma)\frac{\alpha}{2}\sum_{i}\theta_i^2$\\
\textbf{Normal Equation}：$\hat{\theta} = (X^TX)^{-1}X^Ty$

  直接得到权重$\hat{\theta}$，适用于仅有一个输出值的模型

  $X$为 (批量大小, 参数个数)输入矩阵，$y$为(批量大小, )向量
  
  当$X^TX$无逆矩阵时，用psudo inverse$\hat{\theta} = X^+y$\\
\textbf{pseudo inverse}:

  对矩阵$X=USV^T$，pseudo inverse $X^+=VS^+U^T$。$S^+$求法：

  \quad 1.对所有$S$元素，接近0的值赋为0

  \quad 2.对所有非零元素取倒数

  \quad 3.取矩阵转置，得到$S^+$\\
\textbf{log loss}：代价函数

  $J(\theta) = -\frac{1}{|B|}\sum_{i=1}^{|B|}[y^{(i)}log(\hat{p}^{(i)}) + (1-y^{(i)})log(1-\hat{p}^{(i)})]$

  标签值$y^{(i)}$为离散1/0值，计算值$\hat{p}^{(i)} \in [0,1]$

  微分：** 推导 **
  
  \quad $\frac{d J(\theta)}{d \theta_j} = \frac{1}{|B|}\sum_{i=1}^{|B|}(\hat{p}^{(i)} - y^{(i)}) x_j^{(i)}$\\
\textbf{Gaussian Radial Basis Function RBF}：一种similarity function

  $\phi_{\gamma}(x, l) = exp(-\gamma||x-l||^2)$

  \quad $l$为landmark，即$\phi_{\gamma}$由一样本$x_i$和一landmark的距离得来\\
\textbf{Lagrange multipliers method拉格朗日乘数法}

  将\ 有前提的多项式求最值\ 问题转化为\ 无前提多项式最值问题

  定义：

  \quad 对输入向量$X$，$C(X) \geq 0$为constrain。目标为在满足$C(X) \geq 0$的前提下取$f(X)$最值

  \quad Lagrange function $\mathcal{L} (X, \alpha) = f(X) + \alpha(C(X))$

  \quad \quad $\alpha$为变量

  计算：
  
  \quad 对每一$X$的元素\ 和\ $\alpha$取偏导，即向量
  $\begin{bmatrix}
    \frac{d \mathcal{L}(X, \alpha)}{d x_1}  \\
    \frac{d \mathcal{L}(X, \alpha)}{d x_n} \\
    ... \\
    \frac{d \mathcal{L}(X, \alpha)}{d x_n} \\
    \frac{d \mathcal{L}(X, \alpha)}{d \alpha}
  \end{bmatrix}$，计算向量$=\vec{0} $时的$X$, $\alpha$取值




% ----------------------------------------------------------------------
% |                             分类模型                                |
% ----------------------------------------------------------------------
\section{分类模型}
\noindent \textbf{logistic regression}:

  判断输入符合每一输出类别的可能性，

  前向计算：
  
  \quad 1.$\hat{p} = \sigma(\theta^Tx + b)$

  \quad 2.$\hat{y} = 1\space (if \hat{p} \geq 0.5)$

  \quad \quad \quad $= 0\space (if \hat{p} < 0.5)$

  代价函数为log loss\\
\textbf{SVM}

  找到分界，分离多种数据

  support vector: 最靠近分界线的样本

  hard margin classification硬性分类：限制数据必须被分界隔开，同一类数据不可同时出现在分界2端

  soft margin classification：与硬性分类相反，避免被outlier离群值影响

  前向计算：$\hat{p} = f(x_1, x_2,...)$，其余同logistic regression

  \quad 区别：$f$可为polynomial，非线性函数。可使用kernel trick

  线性分类训练：$\hat{p} = W^Tx + b$

  \quad \textbf{硬性分类}：

  \quad \quad $||W||_2$代表线性函数斜率

  \quad \quad 最小化$\frac{1}{2}W^TW$，使得分界平面的斜率最小，最大化分界线和两种数据的距离

  \quad \quad 前提：对每一样本$i$, $1.y^{(i)}\hat{p}^{(i)} \geq 1$，即标签和计算结果相同

  \quad \quad 求解：使用拉格朗日乘数法，其中$\alpha$改为向量，非常数。$\mathcal{L} = \frac{1}{2}W^TW - \sum_{i=1}^{|B|}\alpha^{(i)}(y^{(i)}\hat{p}^{(i)} - 1)$

  \quad \quad \quad 使偏导向量为$\vec{0} $，得到$2.W = \sum_{i=1}^{m}\alpha^{(i)}\hat{p}^{(i)}x^{(i)}$, $3.\sum_{i=1}^{m}\alpha^{(i)}\hat{p}^{(i)}=0$

  \quad \quad \quad 带入得$\mathcal{L} (W, \alpha) = \frac{1}{2}\sum_{i=1}^{|B|}\sum_{j=1}^{|B|}\alpha^{(i)}\alpha^{(j)}\hat{p}^{(i)}\hat{p}^{(j)}x^{(i)T}x^{(j)} - \sum_{i=1}^{|B|}\alpha^{(i)}$，解$\alpha$

  \quad \quad \quad 解$W$：由$\alpha$带入1.式计算

  \quad \quad \quad 解$b$：由于所有support vector$x^{(i)}$ 满足1.式，则对所有support vector计算$b$取平均值

  \quad \quad \quad \quad $b = E(\hat{p}^{(i)}-W^Tx^{(i)})$
  
  \quad \textbf{软性分类}：

  \quad \quad 最小化$\frac{1}{2}W^TW + C\sum_{i=1}^{|B|}\zeta_i $

  \quad \quad \quad $\zeta_i$定义第$i$样本被忽视为误差样本的可能性，$C$定义忽视率相对斜率的权重

  \quad \quad 前提：对每一样本$i$，$y^{(i)}\hat{p}^{(i)} \geq 1 - \zeta^{(i)}$

  非线性分类方法：

  \textbf{- 使用polynomial做$f$}

  \textbf{- 使用similarity function}：
  
  \quad 选择多个landmark$\mathcal{L} = l_1, l_2, ..., l_n$，对每一样本$x_i$计算其和每一$l_j$的$\phi_{\gamma}$值$\phi_{\gamma}(x_i, l_j)$

  \quad 每个样本用新的向量$x_i' = \begin{bmatrix}
    \phi_{\gamma}(x_i, l_1) \\
    \phi_{\gamma}(x_i, l_2) \\
    ... \\
    \phi_{\gamma}(x_i, l_n)
  \end{bmatrix}$表示。新的向量组成训练集，进行SVM训练

  \textbf{kernel}：

  \quad 定义：能够从输入向量$a$,$b$，不通过计算$\phi(a), \phi(b)$直接得到点乘结果$\langle \phi (a), \phi (b)\rangle $的函数

  \quad 例：** 是否通过取linear 为phi得到kernel 函数 **

  \quad \quad linear: $f(a, b) = a^Tb$

  \quad \quad polynomial: $f(a, b) = (\gamma a^Tb+r)^d$

  \quad \quad Gaussian RBF: $f(a, b) = exp(-\gamma ||a-b||^2)$

  \quad \quad Sigmoid: $f(a, b) = tanh(\gamma a^Tb + r)$

% ----------------------------------------------------------------------
% |                              决策树                                 |
% ----------------------------------------------------------------------
\section{决策树}
\noindent 定义：

  节点$N_i$：

  \quad 节点条件：判断样本进入哪一子节点，叶节点没有节点条件
  
  \quad sample属性$S_i$：有多少样本\textbf{进入$N_i$节点}，非满足$N_i$节点条件的样本个数

  \quad value属性$V_i = v_{i1}, ..., v_{in}$：$S_i$进入节点的样本中$v_i$个属于第$i$分类

  \quad gini属性$G_i$：数据混杂度，$G_i = 1-\sum_{j=1}^{n}(\frac{v_{ij}}{S_i})^2$
  
  \quad 子节点仅有2个，对应节点条件为true/false的情况\\
分类方式：数据从根节点开始，根据节点条件传向对应子节点。直到到达叶节点。叶节点中$V$属性中最大项即数据分类\\
\textbf{CART algorithm创建决策树}：

  根节点初始化为叶节点，没有节点条件
  
  对每一叶节点$S_i$选取一特征$k$，一特征门槛$t_k$，将样本集分为2组$S_{true}, S_{false}$。
  
  \quad 选取$(k, t_k)$方式：使代价函数$J(k, t_k) = \frac{S_{true}}{S_i}G_{true} + \frac{S_{false}}{S_i}G_{false}$最小

  直到决策树层数达到固定上限，或对所有分组条件$(k, t_k)$，$J(k, t_k) \geq G_i$\\
\textbf{使用决策树进行regression}

  \begin{figure}[H] %H为当前位置，!htb为忽略美学标准，htbp为浮动图形
    \centering %图片居中
    \includegraphics[width=0.3\textwidth]{note_images/deci_tree_regression.png} %插入图片，[]中设置图片大小，{}中是图片文件名
  \end{figure}

  输入样本，分类进不同值域

  更改：
  
  \quad 每一节点value值为一常数，为$S_i$样本的平均值。
  
  \quad 输出值为叶节点的value，非最大value对应的类别

  \quad $G_i$为$S_i$样本的方差$\frac{1}{S_i}\sum_{j=1}^{S_i}(x_i^{(j)} - \bar{x}_i)^2$

% ----------------------------------------------------------------------
% |                 ensemble learning random forest                    |
% ----------------------------------------------------------------------
\section{ensemble learning \& random forest}
\noindent \textbf{ensemble learning}：使用一组预测机制进行学习，预测机制可为不同算法\\
\textbf{random forest}：

  训练方法：随机选择$n$个训练子集$s_1, s_2, ..., s_n \in S$，训练$n$个决策树$t_1, ..., t_n$。
  
  前向计算：对$n$个树产生的$n$个分类结果，选取投票最多的一分类作为结果
  
  训练子集选取：bagging：子集可重复选取一样本，pasting：样本不重复
  
  \quad out-off-bag oob 样本：当使用bagging选取时，平均只有$1-e^{-1}$样本被选择，余下样本被称为oob样本

  优化：

  \quad random patches随机贴片：对特征和训练集同时取子集进行训练
  
  \quad random subspace随机子空间：对特征取子集，对整个总训练集进行训练
  
  \quad extra-trees极度随机森林：'使用随机$t_k$而不使用最小化数据混杂度的$t_k$'
  
  \quad $k$feature importance特征重要性：对所有取$k$为判断条件的节点$N_i$，计算加权平均值$\sum_i(S_i$imprity降低百分比$)$
  
  \quad (hypothesis) boosting：合并多个预测机制据结果的方法
  
  \quad \quad AdaBoost：串联预测机制，对上一预测机制遗漏的样本加更高权重，进行训练

  \quad \quad gradient boosting\\
\textbf{dimentionality reduction}

  根据manifold assumption，输入训练集纬度较高时在高维空间中训练集参数点稀疏。则将数据压缩到低维

  \textbf{principle component analysis PCA}：

  \quad 对训练集参数矩阵取SVD$USV^T$

  \quad 取$V$中前$d$个向量$V' = [v_1, ..., v_d]$，新训练集$A_{compressed} = A_{origin}V'$
  
  \quad 从\ 新训练集\ 延展回\ 原训练集纬度：$A_{expand} = A_{compressed}V'^T$
  
  \textbf{Incremental PCA}：无需整个训练集存在内存中即可进行SVD
  
  \textbf{kernel PCA}：**
  
  \textbf{local linear Embedding LLE}：
  
  \quad 对每一样本$x^{(i)}$寻找$k$个相邻样本\ 相邻样本index的集合称$C_{x^{(i)}}$
  
  \quad 构建$(|S|, |S|)$矩阵$W$：
  
  \quad \quad 每一行向量$[W_{i1}, ..., W_{i|S|}]$满足$x^{(i)} - \sum_{j \in C_{x^{(i)}}} W_{ij}x^{(j)}$
  
  \quad \quad 每一行向量$W_i$求和为1：$\sum_{i=1}^{|S|}W_i = 1$

  \quad 由$W$创建新训练集：

  \quad \quad 令$z^{(i)}$为$x^{(i)}$在低维的投影

  \quad \quad 使所有$z^{(i)}$满足最小化$(z^{(i)} - \sum_{j=1}^{|B|}w_{ij}z^{(j)})^2$

% ----------------------------------------------------------------------
% |                              无监督学习                                |
% ----------------------------------------------------------------------
\section{无监督学习}
\noindent \textbf{clustering}

  


% ----------------------------------------------------------------------
% |                              分析结果                                |
% ----------------------------------------------------------------------
\section{分析结果}
\noindent \textbf{confusion matrix困惑矩阵}：分析二元/多元分类
  
  $\begin{bmatrix}
    TN & FP \\
    FN & TP
  \end{bmatrix}$

  一行对应同一期望输出，一列对应同一计算输出

  $T/F$: 此位置的计算输出是否和预计输出一致

  $P/N$: 此位置的预计输出是否为真

  \textbf{precision} $= \frac{TP}{TP + FP}$

  \quad 即$P($计算结果匹配 $|$ 计算结果为正$)$

  \textbf{recall} = $\frac{TP}{TP + FN}$

  \quad 即$P($计算结果匹配 $|$ 预计结果为正$)$

  \textbf{$F_1$} $= \frac{2}{\frac{1}{precision} + \frac{1}{recall}}$ 
  
  \quad precision 和 recall的调和平均值
  
  \textbf{specificity} = $\frac{TN}{TN + FN}$\\
\textbf{ROC curve}：分析二元/多元分类

  \begin{figure}[H] %H为当前位置，!htb为忽略美学标准，htbp为浮动图形
    \centering %图片居中
    \includegraphics[width=0.3\textwidth]{note_images/ROC_curve.png} %插入图片，[]中设置图片大小，{}中是图片文件名
  \end{figure}

  y轴recall值，x轴false positive rate$FPR = \frac{FN}{FN + TN} = \frac{FN}{1-specificity}$

  期望的ROC curve为recall从0快速增长到1。并保持直到$FPR$为1。
  
  \quad 即期望曲线下方面积接近1\\
\textbf{learning curves}：观察模型是否有over underfit

  \begin{figure}[H] %H为当前位置，!htb为忽略美学标准，htbp为浮动图形
    \centering %图片居中
    \includegraphics[width=0.3\textwidth]{note_images/learning_curve.png} %插入图片，[]中设置图片大小，{}中是图片文件名
  \end{figure}

  x轴为\textbf{一整次训练(包含多次epoch)使用的训练集大小}，y轴为root MSE。

  画出训练集\ 测试集在使用不同训练集大小后的root MSE。

  分析：

  \quad 期望2曲线平缓值低且相近，

  \quad 当2曲线平缓值差值较大，测试集平缓值较低，则过拟合

  \quad 当2曲线平缓值较高，则欠拟合\\
\textbf{模型复杂度-error epoch-error}：

  \begin{figure}[H] %H为当前位置，!htb为忽略美学标准，htbp为浮动图形
    \centering %图片居中
    \includegraphics[width=0.3\textwidth]{note_images/epoch-complex-error.png} %插入图片，[]中设置图片大小，{}中是图片文件名
  \end{figure}

  2种图，形状类似，x轴内容不同\\

\end{document}
