{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(Q, K, V, mask=None):\n",
    "  # x_q shape: [batch, query_len, query_dim // head] \n",
    "  # x_k shape: [batch, key_len, query_dim // head]\n",
    "  # x_v shape: [batch, key_len, value_dim // head]\n",
    "  # weight shape: [batch, query_len, key_len]\n",
    "  # mask shape: [batch]\n",
    "  weight = torch.bmm(Q, K.permute(0, 2, 1)) / math.sqrt(Q.shape[-1])\n",
    "  if mask is not None:\n",
    "    mask_tensor = torch.arange((weight.shape[-1]))[None, :] < mask[:, None]\n",
    "    weight = weight.masked_fill(~mask_tensor[:, None], -1e6)\n",
    "\n",
    "  return torch.bmm(nn.functional.softmax(weight, dim=-1), V)\n",
    "\n",
    "\n",
    "def split_head(tensor, head):\n",
    "  batch, time, dim = tensor.shape\n",
    "  return tensor.reshape((batch, time, head, dim // head)).permute(2, 0, 1, 3)\n",
    "\n",
    "def scaled_dot_multihead_attention(x_q, x_k, x_v, head_num, mask):\n",
    "  # x_q shape: [batch, query_len, query_dim] \n",
    "  # x_k shape: [batch, key_len, query_dim]\n",
    "  # x_v shape: [batch, key_len, value_dim]\n",
    "  # mask shape: [batch]\n",
    "  x_k = split_head(x_k, head_num)\n",
    "  x_q = split_head(x_q, head_num)\n",
    "  x_v = split_head(x_v, head_num)\n",
    "  # x_q shape: [head, batch, query_len, query_dim // head] \n",
    "  # x_k shape: [head, batch, key_len, query_dim // head]\n",
    "  # x_v shape: [head, batch, key_len, value_dim // head]\n",
    "  output_list = []\n",
    "  for q, k, v in zip(x_q, x_k, x_v):\n",
    "    # output_list element shape: [batch, query_len, attention_dim // head]\n",
    "    output_list.append(attention(q, k, v, mask=mask))\n",
    "\n",
    "  return torch.concat(output_list, dim=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 5.0000, 10.9661, 11.9998, 13.0000, 14.0000],\n",
       "         [10.0000, 11.0000, 12.0000, 13.0000, 14.0000],\n",
       "         [10.0000, 11.0000, 12.0000, 13.0000, 14.0000],\n",
       "         [10.0000, 11.0000, 12.0000, 13.0000, 14.0000]],\n",
       "\n",
       "        [[35.0000, 36.0000, 37.0000, 38.0000, 39.0000],\n",
       "         [35.0000, 36.0000, 37.0000, 38.0000, 39.0000],\n",
       "         [35.0000, 36.0000, 37.0000, 38.0000, 39.0000],\n",
       "         [35.0000, 36.0000, 37.0000, 38.0000, 39.0000]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_hiddens, num_heads = 5, 5\n",
    "batch_size, num_queries, num_kvpairs, valid_lens = 2, 4, 6, torch.tensor([3, 2])\n",
    "\n",
    "q = torch.arange(batch_size * num_queries * num_hiddens, dtype=torch.float32).reshape((batch_size, num_queries, num_hiddens))\n",
    "k = torch.arange(batch_size * num_kvpairs * num_hiddens, dtype=torch.float32).reshape((batch_size, num_kvpairs,num_hiddens))\n",
    "v = torch.arange(batch_size * num_kvpairs * num_hiddens, dtype=torch.float32).reshape((batch_size, num_kvpairs,num_hiddens))\n",
    "\n",
    "# q = torch.ones((batch_size, num_queries, num_hiddens))\n",
    "# k = torch.ones((batch_size, num_kvpairs, num_hiddens))\n",
    "# v = torch.ones((batch_size, num_kvpairs, num_hiddens))\n",
    "\n",
    "scaled_dot_multihead_attention(q, k, v, num_heads, valid_lens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "  def __init__(self, q_dim, k_dim, v_dim, latent_dim, head_num) -> None:\n",
    "    super().__init__()\n",
    "    self.K_pre = nn.Linear(k_dim, latent_dim, bias=False)\n",
    "    self.Q_pre = nn.Linear(q_dim, latent_dim, bias=False)\n",
    "    self.V_pre = nn.Linear(v_dim, latent_dim, bias=False)\n",
    "    self.final = nn.Linear(latent_dim, latent_dim, bias=False)\n",
    "    self.head_num = head_num\n",
    "\n",
    "  def forward(self, x_q, x_k, x_v, mask=None):\n",
    "    # x_k, x_q, x_v shape: [batch, time, latent_dim]\n",
    "    k, q, v = self.K_pre(x_k), self.Q_pre(x_q), self.V_pre(x_v)\n",
    "    attentions = scaled_dot_multihead_attention(q, k, v, self.head_num, mask)\n",
    "    return self.final(attentions)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "  def __init__(self) -> None:\n",
    "      super().__init__()\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.7799,  1.1065,  0.7765, -2.0466,  2.3597],\n",
       "         [ 0.5640,  1.0851,  0.3234, -2.2220,  2.5895],\n",
       "         [ 0.3934,  1.0652, -0.0271, -2.3523,  2.7689],\n",
       "         [ 0.3361,  1.0807, -0.1803, -2.4557,  2.9237]],\n",
       "\n",
       "        [[ 1.7985,  2.4169,  1.1444, -5.5539, 10.9390],\n",
       "         [ 1.7950,  2.4262,  1.1207, -5.5830, 10.9852],\n",
       "         [ 1.7945,  2.4329,  1.1077, -5.6022, 11.0161],\n",
       "         [ 1.7948,  2.4373,  1.1003, -5.6142, 11.0358]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_hiddens, num_heads = 5, 5\n",
    "layer = MultiheadAttention(num_hiddens, num_hiddens, num_hiddens,\n",
    "                               num_hiddens, num_heads)\n",
    "layer.eval()\n",
    "\n",
    "batch_size, num_queries, num_kvpairs, valid_lens = 2, 4, 6, torch.tensor([3, 2])\n",
    "q = torch.arange(batch_size * num_queries * num_hiddens, dtype=torch.float32).reshape((batch_size, num_queries, num_hiddens))\n",
    "k = torch.arange(batch_size * num_kvpairs * num_hiddens, dtype=torch.float32).reshape((batch_size, num_kvpairs,num_hiddens))\n",
    "v = torch.arange(batch_size * num_kvpairs * num_hiddens, dtype=torch.float32).reshape((batch_size, num_kvpairs,num_hiddens))\n",
    "layer(q, k, v, valid_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1f93c46dd61ce6925df2c3958a0a88f8277015b6331ee5021fc1dcace5372220"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('mlenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
